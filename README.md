# ğŸ“š RAG-Powered Domain-Specific Question Answering System

An intelligent assistant that answers questions based on uploaded documents using **RAG (Retrieval-Augmented Generation)** and **LLaMA 3 Instruct 1B**. This project leverages **Haystack** for the backend and **Dash** for a sleek, interactive frontend.

---

## ğŸš€ Demo

![Demo GIF](https://github.com/HeshamEL-Shreif/RAG-powered-Domain-Specific-Question-Answering-System/blob/main/image.png)

---

## âœ¨ Features

- ğŸ“„ Upload multiple files (PDF, DOCX, CSV)
- ğŸ” Chunk, embed, and index documents using Haystack
- ğŸ¤– Ask natural language questions and get accurate answers from LLaMA 3
- ğŸ’¬ Chat-like interface with persistent conversation history
- ğŸŒ Clean UI built using Dash and Bootstrap

---

## ğŸ§  How It Works

1. **Upload Documents:** User uploads one or more documents via the Dash interface.
2. **Text Chunking:** Files are parsed and split into manageable text chunks.
3. **Embedding + Indexing:** Each chunk is embedded and stored in a FAISS vector database.
4. **Query Handling:** User enters a question.
5. **Retrieval + Generation:** Relevant chunks are retrieved using dense vector similarity, then passed to LLaMA 3 Instruct for answer generation.
6. **Conversation Memory:** Stores the chat history for follow-up context.

---

## ğŸ› ï¸ Tech Stack

| Component      | Tooling                                      |
|----------------|----------------------------------------------|
| LLM            | `LLaMA 3 Instruct 1B` from Hugging Face      |
| Framework      | `Haystack` for document processing and RAG   |
| Embeddings     | `SentenceTransformers`     |
| Vector DB      | `FAISS`                                      |
| UI             | `Dash` and `Dash Bootstrap Components`       |
| Backend        | `FastAPI` (optional for scaling/deployment)  |

---

## ğŸ“ Project Structure
rag-pdf-assistant/
â”œâ”€â”€ app/                  # Backend logic
â”‚   â”œâ”€â”€ rag_pipeline.py   # RAG pipeline using Haystack + FAISS
â”‚   â”œâ”€â”€ retrieval.py      # Embedding and chunking logic
â”‚   â””â”€â”€ llm_response.py   # LLM integration (LLaMA 3)
â”‚
â”œâ”€â”€ ui/                   # Dash app
â”‚   â””â”€â”€ app.py            # Main frontend script
â”‚
â”œâ”€â”€ data/                 # Sample uploaded documents
â”œâ”€â”€ tests/                # Unit/integration tests
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ demo.mp4              # Short demo recording

---

## ğŸ§ª Setup & Run

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/rag-pdf-assistant.git
cd rag-pdf-assistant
```
### 2. Install dependencies
``` bash
pip install -r requirements.txt
```
### 3. Run the app
``` bash
python ui/app.py
```

## ğŸ“¦ Requirements
- Python 3.8+
- haystack
- transformers
- sentence-transformers or InstructorEmbedding
- dash, dash-bootstrap-components
- faiss-cpu or faiss-gpu
